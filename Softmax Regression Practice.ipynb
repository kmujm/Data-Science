{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20203074_박정민_과제3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tB3Z7VYQVU1",
        "outputId": "288a8db8-d545-4166-ba1f-e8a69100dad2"
      },
      "source": [
        "import torch\n",
        "\n",
        "# 학습 데이터 생성\n",
        "x_train = torch.FloatTensor([ [1, 2, 1, 1], [2, 1, 3, 2], [3, 1, 3, 4], [4, 1, 5, 5], [1, 7, 5, 5], [1, 2, 5, 6], [1, 6, 6, 6], [1, 7, 7, 7] ])\n",
        "y_train = torch.FloatTensor([ [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0] ])\n",
        "\n",
        "# W, b 초기화\n",
        "# Optimizer 생성\n",
        "W = torch.zeros(4, 3, requires_grad = True)\n",
        "b = torch.zeros(1, 3, requires_grad = True)\n",
        "\n",
        "optimizer = torch.optim.Adam([W, b], lr = 0.1)\n",
        "\n",
        "# 반복 횟수 설정, 가설 및 비용 설정\n",
        "for epoch in range(3001):\n",
        "  hypothesis = torch.softmax(torch.mm(x_train, W) + b, dim = 1)\n",
        "  cost = -torch.mean(torch.sum(y_train * torch.log(hypothesis), dim = 1))\n",
        "\n",
        "  # hypothesis = (torch.mm(x_train, W) + b).softmax(dim = 1)\n",
        "  # cost = -(y_train * torch.log(hypothesis)).sum(dim = 1).mean()\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if epoch % 300 == 0:\n",
        "    print(\"epoch: {}, cost: {:.6f}\".format(epoch, cost.item()))\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, cost: 1.098612\n",
            "epoch: 300, cost: 0.105262\n",
            "epoch: 600, cost: 0.042634\n",
            "epoch: 900, cost: 0.023111\n",
            "epoch: 1200, cost: 0.014479\n",
            "epoch: 1500, cost: 0.009879\n",
            "epoch: 1800, cost: 0.007124\n",
            "epoch: 2100, cost: 0.005338\n",
            "epoch: 2400, cost: 0.004113\n",
            "epoch: 2700, cost: 0.003236\n",
            "epoch: 3000, cost: 0.002588\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG2CjHUILVJ7",
        "outputId": "54d61662-c988-4121-86d6-4be3f378dd6a"
      },
      "source": [
        "W.requires_grad_(False)\n",
        "b.requires_grad_(False)\n",
        "\n",
        "x_test = torch.FloatTensor([[1, 11, 10, 9], [1, 3, 4, 3], [1, 1, 0, 1]])\n",
        "test_all = torch.softmax(torch.mm(x_test, W) + b, dim = 1)\n",
        "print(test_all)\n",
        "print(torch.argmax(test_all, dim = 1))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000e+00, 5.5165e-19, 7.0151e-38],\n",
            "        [1.4800e-02, 7.4294e-01, 2.4226e-01],\n",
            "        [1.2256e-33, 9.0835e-12, 1.0000e+00]])\n",
            "tensor([0, 1, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axzKWCigN914",
        "outputId": "10a59663-7303-41ce-d525-d09c3cd66c12"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# 학습 데이터 생성\n",
        "x_train = torch.FloatTensor([ [1, 2, 1, 1], [2, 1, 3, 2], [3, 1, 3, 4], [4, 1, 5, 5], [1, 7, 5, 5], [1, 2, 5, 6], [1, 6, 6, 6], [1, 7, 7, 7] ])\n",
        "y_train = torch.LongTensor([2, 2, 2, 1, 1, 1, 0, 0])\n",
        "\n",
        "# W, b 초기화\n",
        "# Optimizer 생성\n",
        "W = torch.zeros(4, 3, requires_grad = True)\n",
        "b = torch.zeros(1, 3, requires_grad = True)\n",
        "\n",
        "optimizer = torch.optim.Adam([W, b], lr = 0.1)\n",
        "\n",
        "# 반복 횟수 설정, 가설 및 비용 설정\n",
        "for epoch in range(3001):\n",
        "  z = torch.mm(x_train, W) + b\n",
        "  cost = F.cross_entropy(z, y_train)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if epoch % 300 == 0:\n",
        "    print(\"epoch: {}, cost: {:.6f}\".format(epoch, cost.item()))\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, cost: 1.098612\n",
            "epoch: 300, cost: 0.105263\n",
            "epoch: 600, cost: 0.042634\n",
            "epoch: 900, cost: 0.023111\n",
            "epoch: 1200, cost: 0.014479\n",
            "epoch: 1500, cost: 0.009879\n",
            "epoch: 1800, cost: 0.007124\n",
            "epoch: 2100, cost: 0.005338\n",
            "epoch: 2400, cost: 0.004113\n",
            "epoch: 2700, cost: 0.003236\n",
            "epoch: 3000, cost: 0.002588\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z66VgfEVQiYX",
        "outputId": "6e4d96f2-5ec0-42bc-d845-7c58ff3e5bec"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# 학습 데이터 생성\n",
        "x_train = torch.FloatTensor([ [1, 2, 1, 1], [2, 1, 3, 2], [3, 1, 3, 4], [4, 1, 5, 5], [1, 7, 5, 5], [1, 2, 5, 6], [1, 6, 6, 6], [1, 7, 7, 7] ])\n",
        "y_train = torch.LongTensor([2, 2, 2, 1, 1, 1, 0, 0])\n",
        "\n",
        "model = nn.Linear(4, 3)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1)\n",
        "\n",
        "# 반복 횟수 설정, 가설 및 비용 설정\n",
        "for epoch in range(3001):\n",
        "  z = model(x_train)\n",
        "  cost = F.cross_entropy(z, y_train)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if epoch % 300 == 0:\n",
        "    print(\"epoch: {}, cost: {:.6f}\".format(epoch, cost.item()))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, cost: 4.672721\n",
            "epoch: 300, cost: 0.028212\n",
            "epoch: 600, cost: 0.010680\n",
            "epoch: 900, cost: 0.005689\n",
            "epoch: 1200, cost: 0.003547\n",
            "epoch: 1500, cost: 0.002418\n",
            "epoch: 1800, cost: 0.001744\n",
            "epoch: 2100, cost: 0.001308\n",
            "epoch: 2400, cost: 0.001009\n",
            "epoch: 2700, cost: 0.000794\n",
            "epoch: 3000, cost: 0.000636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pCcaD9fRX27",
        "outputId": "67f45ebf-2636-46ea-9f69-f12fe16615a5"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "x_train = torch.FloatTensor([ [1, 2, 1, 1], [2, 1, 3, 2], [3, 1, 3, 4], [4, 1, 5, 5], [1, 7, 5, 5], [1, 2, 5, 6], [1, 6, 6, 6], [1, 7, 7, 7] ])\n",
        "y_train = np.array([2, 2, 2, 1, 1, 1, 0, 0])\n",
        "\n",
        "logistic = LogisticRegression()\n",
        "logistic.fit(x_train, y_train)\n",
        "\n",
        "pred = logistic.predict([[1, 11, 10, 9], [1, 3, 4, 3], [1, 1, 0, 1]])\n",
        "print(pred)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 2 2]\n"
          ]
        }
      ]
    }
  ]
}